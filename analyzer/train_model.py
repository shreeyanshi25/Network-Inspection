import numpy as np
import pandas as pd
import joblib
import os
import urllib.request

from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

# ==============================
# NSL-KDD Dataset Download
# ==============================

DATASET_URL = (
    "https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain%2B.txt"
)
DATASET_PATH = "KDDTrain+.txt"

NSL_KDD_COLUMNS = [
    "duration", "protocol_type", "service", "flag",
    "src_bytes", "dst_bytes", "land", "wrong_fragment", "urgent", "hot",
    "num_failed_logins", "logged_in", "num_compromised", "root_shell",
    "su_attempted", "num_root", "num_file_creations", "num_shells",
    "num_access_files", "num_outbound_cmds", "is_host_login", "is_guest_login",
    "count", "srv_count", "serror_rate", "srv_serror_rate", "rerror_rate",
    "srv_rerror_rate", "same_srv_rate", "diff_srv_rate", "srv_diff_host_rate",
    "dst_host_count", "dst_host_srv_count", "dst_host_same_srv_rate",
    "dst_host_diff_srv_rate", "dst_host_same_src_port_rate",
    "dst_host_srv_diff_host_rate", "dst_host_serror_rate",
    "dst_host_srv_serror_rate", "dst_host_rerror_rate",
    "dst_host_srv_rerror_rate", "label", "difficulty"
]

print("=" * 55)
print("  Traffic Intelligence â€” ML Model Trainer")
print("  Using NSL-KDD Network Intrusion Dataset")
print("=" * 55)

# â”€â”€ Step 1: Download Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not os.path.exists(DATASET_PATH):
    print(f"\nğŸ“¥ Downloading NSL-KDD dataset...")
    try:
        urllib.request.urlretrieve(DATASET_URL, DATASET_PATH)
        print("âœ… Dataset downloaded successfully!\n")
    except Exception as e:
        print(f"âš ï¸  Download failed ({e}). Using realistic synthetic fallback.\n")
        DATASET_PATH = None
else:
    print(f"\nâœ… Dataset already present: {DATASET_PATH}\n")

# â”€â”€ Step 2: Build Feature Matrix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# Feature mapping from NSL-KDD â†’ our 6 packet-level features:
#   packet_count â† count (connection count to same host in 2s window)
#   total_bytes  â† src_bytes + dst_bytes
#   avg_size     â† total_bytes / max(count, 1)
#   duration     â† duration (in seconds)
#   byte_rate    â† total_bytes / max(duration, 0.001)
#   packet_rate  â† count / max(duration, 0.001)
#

if DATASET_PATH and os.path.exists(DATASET_PATH):
    print("ğŸ“Š Loading NSL-KDD dataset...")
    df = pd.read_csv(DATASET_PATH, header=None, names=NSL_KDD_COLUMNS)
    print(f"   Rows loaded : {len(df):,}")
    print(f"   Normal flows: {(df['label'] == 'normal').sum():,}")
    print(f"   Attack flows: {(df['label'] != 'normal').sum():,}\n")

    # Cast numeric columns
    for col in ["duration", "src_bytes", "dst_bytes", "count",
                "srv_count", "dst_host_count"]:
        df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0)

    df["total_bytes"]  = df["src_bytes"] + df["dst_bytes"]
    df["avg_size"]     = df["total_bytes"] / df["count"].clip(lower=1)
    df["byte_rate"]    = df["total_bytes"] / df["duration"].clip(lower=0.001)
    df["packet_rate"]  = df["count"]       / df["duration"].clip(lower=0.001)
    df["packet_count"] = df["count"]

    feature_cols = [
        "packet_count", "total_bytes", "avg_size",
        "duration", "byte_rate", "packet_rate"
    ]

    # â”€â”€ Training: use ONLY normal (benign) flows â”€â”€
    # Isolation Forest is unsupervised â€” trained on normal data only,
    # it learns to flag anomalous patterns as outliers.
    normal_df = df[df["label"] == "normal"][feature_cols].copy()
    normal_df = normal_df.clip(lower=0).fillna(0)

    # Cap extreme outliers at 99th percentile so scaler isn't skewed
    for col in feature_cols:
        cap = normal_df[col].quantile(0.99)
        normal_df[col] = normal_df[col].clip(upper=cap)

    X = normal_df.values
    print(f"âœ… Feature matrix ready: {X.shape[0]:,} normal flows Ã— {X.shape[1]} features")

else:
    # â”€â”€ Synthetic fallback (realistic distributions) â”€â”€â”€â”€â”€â”€
    print("ğŸ”„ Generating realistic synthetic training data as fallback...")

    normal_traffic = np.random.normal(
        loc=[20, 5000, 250, 5, 1000, 5],
        scale=[5, 2000, 50, 2, 500, 2],
        size=(1000, 6)
    ).clip(min=0)

    heavy_traffic = np.random.normal(
        loc=[200, 500_000, 1400, 20, 20_000, 20],
        scale=[50, 200_000, 300, 5, 10_000, 5],
        size=(200, 6)
    ).clip(min=0)

    X = np.vstack([normal_traffic, heavy_traffic])
    print(f"âœ… Synthetic matrix ready: {X.shape[0]} samples Ã— {X.shape[1]} features")

# â”€â”€ Step 3: Scale â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nâš™ï¸  Fitting StandardScaler...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# â”€â”€ Step 4: Train Isolation Forest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸŒ² Training Isolation Forest...")
print("   n_estimators : 300")
print("   contamination: 0.05  (5% anomaly rate)")
print("   max_features : 1.0")

model = IsolationForest(
    n_estimators=300,
    contamination=0.05,
    max_features=1.0,
    bootstrap=True,
    random_state=42,
    n_jobs=-1
)
model.fit(X_scaled)

# â”€â”€ Step 5: Save Artifacts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
joblib.dump(model,  "anomaly_model.pkl")
joblib.dump(scaler, "scaler.pkl")

print("\nâœ… Model saved  â†’ anomaly_model.pkl")
print("âœ… Scaler saved â†’ scaler.pkl")
print("\nğŸ‰ Training complete! Re-run packet_capture.py to use the new model.")
print("=" * 55)